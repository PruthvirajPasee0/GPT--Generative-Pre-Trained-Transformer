
# GPT--Generative-Pre-Trained-Transformer

This repository contains a very small, low level implementation of a LLM. The main purpose of this project was to understand complex mechanisms behind a LLM. It isn't really useful at the moment as it generates some shakespearean rubbish.


# 
## Related

Here are some the links to some collab files that I used in this project:

File used experimentations and learning: https://colab.research.google.com/drive/1i2WrssJFRj8Z8s63k0QGppoDd90gr5YS?usp=sharing

Final project compatible with collab (as I don't have a GPU available): https://colab.research.google.com/drive/1r7iwR70DWwD7xZ8J0450i2RbiQGKx4H6?usp=sharing

The transformer architecture being used in this project:
<img width="650" height="834" alt="Transformer_Architecture" src="https://github.com/user-attachments/assets/2896fc1a-3049-4793-b87a-a5c5f80177ab" />



## üôè Acknowledgments
- Andrej Karpathy. (https://youtu.be/kCc8FmEb1nY?si=gN-Y2AjL0pox1oYm)
- Attention is all you need. (https://arxiv.org/abs/1706.03762)
